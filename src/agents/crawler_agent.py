import requests
import json
import logging
import os
import time
from src.config import LOG_FILES, TITLES_FILE, RAW_FILE

# Configuraci√≥n de logging
LOG_FILE = LOG_FILES["crawler"]

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

WIKI_API_URL = "https://es.wikipedia.org/w/api.php"
INPUT_FILE = TITLES_FILE
OUTPUT_FILE = RAW_FILE


def safe_get(url, params, max_retries=3, timeout=10):
    """
    Realiza una solicitud GET con reintentos y manejo de errores.

    Args:
        url (str): URL base de la API.
        params (dict): Par√°metros de la solicitud.
        max_retries (int): N√∫mero m√°ximo de reintentos.
        timeout (int): Tiempo m√°ximo de espera por solicitud (segundos).

    Returns:
        dict or None: Respuesta JSON si es exitosa; None en caso de fallo.
    """
    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            logging.info(f"‚úÖ GET exitoso en intento {attempt} para {params}")
            return response.json()
        except requests.exceptions.RequestException as e:
            logging.warning(f"‚ö†Ô∏è Intento {attempt} fall√≥: {e}")
            if attempt < max_retries:
                time.sleep(2 * attempt)
    logging.error(f"‚ùå Fallo tras {max_retries} intentos para URL: {url} con params: {params}")
    return None


def search_article(query):
    """
    Busca el t√≠tulo real de un art√≠culo en Wikipedia para un t√©rmino dado.

    Args:
        query (str): T√©rmino de b√∫squeda.

    Returns:
        str or None: T√≠tulo del primer resultado o None si no se encuentra.
    """
    logging.info(f"üîé Buscando art√≠culo para query: '{query}'")
    params = {
        "action": "query",
        "format": "json",
        "list": "search",
        "srsearch": query
    }
    resp = safe_get(WIKI_API_URL, params)
    if not resp:
        logging.error(f"‚ùå No se obtuvo respuesta de la API para: '{query}'")
        return None
    search_results = resp.get("query", {}).get("search", [])
    if search_results:
        title = search_results[0]["title"]
        logging.info(f"‚úÖ Art√≠culo encontrado: '{title}' para query '{query}'")
        return title
    logging.warning(f"‚ö†Ô∏è No se encontraron resultados para: '{query}'")
    return None


def fetch_article_data(title):
    """
    Obtiene el extracto y las categor√≠as de un art√≠culo de Wikipedia.

    Args:
        title (str): T√≠tulo del art√≠culo.

    Returns:
        tuple: Extracto (str) y lista de categor√≠as (list).
    """
    logging.info(f"üìÑ Descargando datos del art√≠culo: '{title}'")
    params_extract = {
        "action": "query",
        "format": "json",
        "prop": "extracts|categories",
        "titles": title,
        "explaintext": 1,
        "cllimit": "max"
    }
    resp = safe_get(WIKI_API_URL, params_extract)
    extract = ""
    categories = []
    if resp:
        pages = resp.get("query", {}).get("pages", {})
        for page in pages.values():
            extract = page.get("extract", "")
            category_list = page.get("categories", [])
            categories = [
                cat.get("title", "").replace("Categor√≠a:", "").strip()
                for cat in category_list
                if not cat.get("title", "").startswith("Categor√≠a:Wikipedia:")
            ]
    logging.info(f"‚úÖ Datos obtenidos para '{title}' (Extracto: {len(extract)} caracteres, Categor√≠as: {len(categories)})")
    return extract, categories


def process_article(query, title):
    """
    Procesa un art√≠culo: obtiene su extracto y categor√≠as.

    Args:
        query (str): T√©rmino original de b√∫squeda.
        title (str): T√≠tulo del art√≠culo.

    Returns:
        dict: Diccionario con los datos del art√≠culo.
    """
    logging.info(f"üöÄ Procesando art√≠culo: '{title}' para query: '{query}'")
    extract, categories = fetch_article_data(title)
    return {
        "query": query,
        "title": title,
        "content": extract,
        "categories": categories
    }


def load_existing_articles(output_file):
    """
    Carga los art√≠culos previamente almacenados.

    Args:
        output_file (str): Ruta al archivo JSON de salida.

    Returns:
        list: Lista de art√≠culos existentes.
    """
    if not os.path.exists(output_file):
        logging.info("‚ÑπÔ∏è No se encontr√≥ archivo previo. Se iniciar√° nuevo corpus.")
        return []
    with open(output_file, "r", encoding="utf-8") as f:
        logging.info(f"üìÇ Archivo existente cargado: {output_file}")
        return json.load(f)


def save_articles(output_file, articles):
    """
    Guarda los art√≠culos en un archivo JSON.

    Args:
        output_file (str): Ruta al archivo de salida.
        articles (list): Lista de art√≠culos a guardar.
    """
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    logging.info(f"üíæ Corpus guardado en: {output_file} (Total art√≠culos: {len(articles)})")


def crawl_titles(input_file=INPUT_FILE, output_file=OUTPUT_FILE):
    """
    Realiza crawling para una lista de t√≠tulos.

    Args:
        input_file (str): Ruta al archivo con queries.
        output_file (str): Ruta al archivo de salida.
    """
    logging.info("üöÄ Inicio del crawling por t√≠tulos")
    with open(input_file, "r", encoding="utf-8") as f:
        topics = json.load(f)

    existing_articles = load_existing_articles(output_file)
    existing_titles = {a["title"] for a in existing_articles}
    results = existing_articles.copy()

    for item in topics:
        query = item["query"]
        title = search_article(query)
        if not title:
            continue
        if title in existing_titles:
            logging.info(f"‚è© Art√≠culo ya existente: '{title}'")
            continue
        result = process_article(query, title)
        results.append(result)
        existing_titles.add(title)

    save_articles(output_file, results)
    logging.info(f"üèÅ Proceso completado. Total temas procesados: {len(results)}")


def crawl_single_title(query, output_file=OUTPUT_FILE):
    """
    Procesa un solo t√©rmino, descarga y guarda el art√≠culo si es nuevo.

    Args:
        query (str): T√©rmino de b√∫squeda.
        output_file (str): Ruta al archivo de salida.

    Returns:
        dict or None: Art√≠culo procesado o None si no fue a√±adido.
    """
    logging.info(f"üåê Crawler intentando buscar art√≠culo para: '{query}'")
    existing_articles = load_existing_articles(output_file)
    existing_titles = {a["title"] for a in existing_articles}
    title = search_article(query)
    if not title:
        return None
    if title in existing_titles:
        logging.info(f"‚è© Art√≠culo '{title}' ya existe en el corpus.")
        return None
    result = process_article(query, title)
    existing_articles.append(result)
    save_articles(output_file, existing_articles)
    logging.info(f"‚úÖ Art√≠culo '{title}' a√±adido al corpus.")
    return result


if __name__ == "__main__":
    crawl_titles(INPUT_FILE, OUTPUT_FILE)
